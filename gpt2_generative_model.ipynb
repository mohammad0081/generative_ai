{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI with GPT2\n",
        "\n",
        "* Developed by MOHAMMAD HASSAN HEYDARI"
      ],
      "metadata": {
        "id": "Rhhg0rJSfqrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rRE2mUbxfpvT"
      },
      "outputs": [],
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize the tokenizer and the model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Bn7AIJgbMK",
        "outputId": "4cf1cced-c085-4beb-fcd2-5dcb5643c7b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLay  multiple                  124439808 \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 124439808 (474.70 MB)\n",
            "Trainable params: 124439808 (474.70 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='tf')\n",
        "\n",
        "    # Generate a sequence of tokens\n",
        "    outputs = model.generate(inputs, max_length=200, temperature=0.1, num_return_sequences=1)\n",
        "\n",
        "    # Decode the sequence of tokens to get the generated text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "taPXWYiMgAbN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "print(generate_text(\"once upon a time in middle east\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS8YdShtgEyC",
        "outputId": "9135fc07-5994-46e0-f438-f03f711d4925"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time in middle east history when the Ottoman Empire was in its infancy.\n",
            "\n",
            "The Ottoman Empire was a powerful and powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was able to control the world's resources and influence. It was a powerful empire that was\n"
          ]
        }
      ]
    }
  ]
}